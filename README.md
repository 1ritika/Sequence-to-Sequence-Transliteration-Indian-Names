#  Sequence-to-Sequence Transliteration â€” English â†’ Hindi

This project implements **character-level neural sequence-to-sequence models** for transliterating **Indian names** from English to Hindi.  
It explores and compares **RNN (GRU)** and **GRU + Attention** architectures, evaluating translation quality using BLEU metrics and decoding strategies.

---

## ğŸš€ Features
- Implemented **Encoderâ€“Decoder GRU** model for character-level transliteration.  
- Extended with **Bahdanau Attention**, improving alignment and contextual mapping.  
- Integrated both **greedy** and **beam-search decoding** for output sequence generation.  
- Achieved **BLEU score = 0.73** with Attention + Beam Search â€” significantly outperforming the vanilla GRU baseline.  
- Visualized attention weights to interpret inputâ€“output alignment.

---

## ğŸ§© Dataset
- **Indian name transliteration dataset** (Englishâ€“Hindi pairs).  
- Preprocessed to handle Unicode normalization and tokenization at character level.  
- Split into **training / validation / test** sets for model evaluation.

---

## ğŸ§  Models Implemented
| Model | Architecture | Decoding | BLEU |
|--------|---------------|----------|------|
| GRU Encoderâ€“Decoder | 1-layer GRU | Greedy | 0.65 |
| GRU + Attention | Bahdanau Attention | Beam Search | **0.73** |

---

## ğŸ“ˆ Results
- Attention-based model demonstrated **better sequence alignment** and reduced transliteration errors.  
- Beam Search decoding improved final output fluency and accuracy.  
- Attention visualizations revealed interpretable source-target mappings.

---

## ğŸ› ï¸ Requirements
```bash
pip install torch numpy matplotlib tqdm
